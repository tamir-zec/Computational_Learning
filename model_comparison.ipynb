{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computional Learning - Assignment 3\n",
    "\n",
    "Tamir Zecler 204168223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part explains how I preprocessed the data and gives a feel to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "in order to load the dataset properly, please make sure that data set path specified is relevent. \n",
    "My code assumes the data set is in the same directory as this notebook. it splits data to label and features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ex3_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, ignore_nan=True):\n",
    "  df = pd.read_csv(path)\n",
    "  entries = df.shape[0]\n",
    "  if ignore_nan == True:\n",
    "      df = df.dropna()\n",
    "  return df\n",
    "\n",
    "def split_x_y_df(df):\n",
    "  y_data = df['EVENT_PRIMARY']\n",
    "  x_data = df.drop(['EVENT_PRIMARY'], axis=1)\n",
    "  return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, y_df = split_x_y_df(load_data(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our data constists of 8281 entries with 29 features each\n"
     ]
    }
   ],
   "source": [
    "print(f'our data constists of {x_df.shape[0]} entries with {x_df.shape[1]} features each')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome:: Dead: 513, Alive:7768\n",
      "Death Precentage: 6.194903997101799\n"
     ]
    }
   ],
   "source": [
    "print(f'Outcome:: Dead: {sum(y_df)}, Alive:{y_df.shape[0]-sum(y_df)}')\n",
    "print(f'Death Precentage: {100*sum(y_df)/y_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Types:\n",
    "in this data set we have 29 different features we can use to predict, out of those features we have:\n",
    "<ul>\n",
    "    <li>11 - boolean features</li>\n",
    "    <li>2 - Categorical Features</li>\n",
    "    <li>16 - Numeric Features</li>\n",
    "</ul>\n",
    "\n",
    "first and most importantly at this point, is to make data more learnable.\n",
    "in order to do that I used a min-max scaler on the numeric values, as well as \n",
    "using dummy variables for categorical endocing of features since there is no meaning to order on both of them(infered from assignment features explination, could be missunderstanding of clinical arm explanation).\n",
    "also, I choose to ignore RACE_BLACK feature since its already depicted in the race_4(same column for black after dummy variable use) and just feels racist, as well as site location, since we hope to get results that are not dependent on location for robustness, trying to use it as a feature seems redundant. during trail and error adding\\removing features to yield best results\n",
    "for the boolean features I chose to not one hot encode them as it more likely to suffer from curse of dimensionality, as well as consumes more memory and can cause more overfitting then in normal represantions, since I cant understand each feature meaning, I choose to limit the changes I make based on previous knowledge. I choose to use min-max scaler with deafult value range so it wont change my binary features. since values are positive only the is no reason to prefer MaxAbsScaler. I used min max scaler as I have no previous knowledge and usinging a standard range of 0 1 to learn gives every feature similiar 'weight'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ignored = ['RACE_BLACK', 'NEWSITEID']\n",
    "categorical_features = ['RACE4', 'INTENSIVE']\n",
    "boolean_features = ['INCLUSIONFRS', 'NOAGENTS', 'ASPIRIN', 'SUB_CKD', 'FEMALE', 'SUB_CVD', 'SUB_CLINICALCVD', 'SUB_SUBCLINICALCVD', 'SUB_SENIOR', 'STATIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(x_df, y_df):\n",
    "    # drop unused features:\n",
    "    x_df = x_df.drop(columns=features_ignored)\n",
    "    \n",
    "    # make sure boolean values are ints\n",
    "    y_df = y_df.astype(int)\n",
    "    for bool_feat in boolean_features:\n",
    "            x_df[[bool_feat]] = x_df[[bool_feat]].astype(int)\n",
    "\n",
    "    # make dummy variable from categorical features\n",
    "    for category in categorical_features:\n",
    "        dummies = pd.get_dummies(x_df[[category]])\n",
    "        x_df.drop([category], axis=1, inplace=True)\n",
    "        x_df = pd.concat([x_df, dummies], axis=1)  \n",
    "    \n",
    "    #scale the feature values with min max scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(x_df) \n",
    "    # this returns a numpy array and not a data frame\n",
    "    x_ndr = scaler.transform(x_df)\n",
    "    y_ndr = y_df.to_numpy()\n",
    "    return x_ndr, y_ndr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Provided Dataset is heavily unbalanced, as seen after loading, only about 6% of the labels are labeled as death.\n",
    "In order to overcome this problem, oversampling method was used, I used the RandomOverSampler from the imblearn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lm = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear classifier I choose to work with is the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enesmble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_em = RandomForestClassifier() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model I choose to use is the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_dl = MLPClassifier(solver='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to use adam as optimizer since its considered a standard and each model takes a long time to train so I focused on other hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "In order to get best hyper parameters, grid search for each model was used. I used to use imblearn pipeline since it has better support for over sampling in its pipeline, during fit stage it does it by itself. In general since this data set is not balanced, using the AUC can help us get an understanding of the probabilty that a random positive sample gets a higher score than a random negetive score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = load_data(dataset_path)\n",
    "x_df, y_df = split_x_y_df(dataset_df)\n",
    "x_ndr_prcsd, y_ndr_prcsd = data_preprocess(x_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_gridSearch(x_data, y_data, resampling=False, scoring='recall'):\n",
    "  lm_parameters = {\"tol\": [0.001, .0025, 0.005, 0.01, 0.025], 'solver': ['newton-cg', 'lbfgs', 'saga']}\n",
    "  clf_lm_grid = GridSearchCV(clf_lm, lm_parameters, scoring=scoring)\n",
    "  if resampling:\n",
    "      sampler = RandomOverSampler()\n",
    "      pipeline = Pipeline([(\"sampler\", sampler),\n",
    "                           (\"classifier\", clf_lm_grid)])\n",
    "      pipeline.fit(x_data, y_data)\n",
    "      print(\"Best parameters for Logistic Regression with Resampling:\")\n",
    "      print(clf_lm_grid.best_params_)\n",
    "  else:\n",
    "      clf_lm_grid.fit(x_data, y_data)\n",
    "      print(\"Best parameters for Logistic Regression without Resampling:\")   \n",
    "      print(clf_lm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression without Resampling:\n",
      "{'solver': 'newton-cg', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression without Resampling:\n",
      "{'solver': 'newton-cg', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_df_prcsd, y_df_prcsd, resampling=False, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression without Resampling:\n",
      "{'solver': 'lbfgs', 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=False, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression with Resampling:\n",
      "{'solver': 'saga', 'tol': 0.025}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression with Resampling:\n",
      "{'solver': 'saga', 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression with Resampling:\n",
      "{'solver': 'newton-cg', 'tol': 0.0025}\n"
     ]
    }
   ],
   "source": [
    "lm_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am going to use oversampling in my model fitting, I choose parameters more based on them, as well as the default recall I choose for it since It feels more important to have a good value on  recall when trying to predict the death. Overall, the paramaters I am going to use are: {'solver': 'saga', 'tol': 0.002}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_gridSearch(x_data, y_data, resampling=False, scoring='recall'):\n",
    "  em_parameters = {'n_estimators': [10,50,100,250], 'criterion': ['gini', 'entropy'], 'max_samples': [0.5, 0.625, 0.75, 0.825, 1], 'max_depth': [3,5,7,9]}\n",
    "  clf_em_grid = GridSearchCV(clf_em, em_parameters, scoring=scoring)\n",
    "  if resampling:\n",
    "      sampler = RandomOverSampler()\n",
    "      pipeline = Pipeline([(\"sampler\", sampler), (\"classifier\", clf_em_grid)])\n",
    "      pipeline.fit(x_data, y_data)\n",
    "      print(\"Best parameters for Random Forest with Resampling:\")\n",
    "      print(clf_em_grid.best_params_)\n",
    "  else:\n",
    "      clf_em_grid.fit(x_data, y_data)\n",
    "      print(\"Best parameters for Random Forest without Resampling:\")   \n",
    "      print(clf_em_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest without Resampling:\n",
      "{'criterion': 'gini', 'max_depth': 9, 'max_samples': 0.75, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest without Resampling:\n",
      "{'criterion': 'gini', 'max_depth': 9, 'max_samples': 0.825, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=False, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest without Resampling:\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_samples': 0.825, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=False, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Resampling:\n",
      "{'criterion': 'entropy', 'max_depth': 9, 'max_samples': 1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Resampling:\n",
      "{'criterion': 'gini', 'max_depth': 9, 'max_samples': 0.75, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Resampling:\n",
      "{'criterion': 'gini', 'max_depth': 9, 'max_samples': 0.825, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "em_best_parms = em_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As We can see, when using resampling the preferd paramaters are higher sample rate, higher depth and higher number of estimators, I will use the following parameters: {'criterion': 'entropy', 'max_depth': 9, 'max_samples': 0.9, 'n_estimators': 100}\n",
    "since I am going to resample and Recall feels like the most netural metrics to choose by, since it gives us a feel on positive labels predictions - which is the main goal in my opnion when trying to class death cases, My choices were mainly based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Model\n",
    "in this model i tried relu and sigmoid for activation but choose while considering the calcultion efficiency as well.\n",
    "I tried only a few network architectures and alpha (penalty weight) since each model is expensive to compute, as well as only checked with resampling since I am going to use it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_gridSearch(x_data, y_data, resampling=False, scoring='recall'):\n",
    "    dl_paramaters = {\n",
    "    'hidden_layer_sizes': [(7,3,5), (5,5,3,2), (10,10)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'alpha': [0.0001, 0.005]\n",
    "    }\n",
    "    clf_dl_grid = GridSearchCV(clf_dl, dl_paramaters, scoring=scoring)\n",
    "    if resampling:\n",
    "      sampler = RandomOverSampler()\n",
    "      pipeline = Pipeline([(\"sampler\", sampler), (\"classifier\", clf_dl_grid)])\n",
    "      pipeline.fit(x_data, y_data)\n",
    "      print(\"Best parameters for Neural Network with Resampling:\")\n",
    "      print(clf_dl_grid.best_params_)\n",
    "    else:\n",
    "      clf_dl_grid.fit(x_data, y_data)\n",
    "      print(\"Best parameters for  Neural Network without Resampling:\")   \n",
    "      print(clf_dl_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network with Resampling:\n",
      "{'activation': 'relu', 'alpha': 0.005, 'hidden_layer_sizes': (7, 3, 5)}\n"
     ]
    }
   ],
   "source": [
    "dl_best_parms = dl_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network with Resampling:\n",
      "{'activation': 'relu', 'alpha': 0.005, 'hidden_layer_sizes': (10, 10)}\n"
     ]
    }
   ],
   "source": [
    "dl_best_parms = dl_gridSearch(x_ndr_prcsd, y_ndr_prcsd, resampling=True, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we couldnt reach an agreement on architecture I used the one with better recall, that is:\n",
    "{'activation': 'relu', 'alpha': 0.005, 'hidden_layer_sizes': (7, 3, 5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the StartifiedKFold to ensure balanced distribution of test train split label wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a 10-fold cross validation in these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_results():\n",
    "    def __init__(self, classifier_name):\n",
    "        self.classifier_name = classifier_name\n",
    "        self.acc = []\n",
    "        self.auc = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1 = []\n",
    "        \n",
    "    def print_eval(self):\n",
    "        mean_acc = sum(self.acc) / len(self.acc)\n",
    "        mean_auc = sum(self.auc) / len(self.auc)\n",
    "        mean_precision = sum(self.precision) / len(self.precision)\n",
    "        mean_recall = sum(self.recall) / len(self.recall)\n",
    "        mean_f1 = sum(self.f1) / len(self.f1)\n",
    "        print(f'{self.classifier_name} - Accuracy: {mean_acc} AUC: {mean_auc} Percision: {mean_precision} Recall: {mean_recall} F1: {mean_f1}')\n",
    "        \n",
    "    def add_evaluation(self, y_test, y_pred): \n",
    "      self.acc.append(accuracy_score(y_test, y_pred))\n",
    "      self.auc.append(roc_auc_score(y_test, y_pred))\n",
    "      self.precision.append(precision_score(y_test, y_pred))\n",
    "      self.recall.append(recall_score(y_test, y_pred))\n",
    "      self.f1.append(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(x_ndr, y_ndr, clf_lm, clf_em, clf_dl):\n",
    "    num_splits = 10\n",
    "    cross_validator = StratifiedKFold(n_splits=num_splits)\n",
    "    sampler =RandomOverSampler()\n",
    "    lm_classifier_results = classifier_results('Logistic Regression')\n",
    "    em_classifier_results = classifier_results('Random Forest')\n",
    "    dl_classifier_results = classifier_results('Multi Layer Percepton')\n",
    " \n",
    "    # Training loop\n",
    "        \n",
    "    for i, (train_idxs, test_idxs) in enumerate(cross_validator.split(x_ndr, y_ndr)):\n",
    "        print(f'iteration {i+1} out of {num_splits} iterations.')\n",
    "        x_train, y_train = x_ndr[train_idxs], y_ndr[train_idxs]\n",
    "        x_test, y_test = x_ndr[test_idxs], y_ndr[test_idxs]\n",
    "        \n",
    "        x_train_resample, y_train_resample= sampler.fit_resample(x_train, y_train)\n",
    "        \n",
    "        clf_lm.fit(x_train_resample, y_train_resample)\n",
    "        clf_em.fit(x_train_resample, y_train_resample)\n",
    "        clf_dl.fit(x_train_resample, y_train_resample)\n",
    "        \n",
    "        \n",
    "        clf_lm_pred = clf_lm.predict(x_test)\n",
    "        clf_em_pred = clf_em.predict(x_test)\n",
    "        clf_dl_pred = clf_dl.predict(x_test)\n",
    "        \n",
    "        lm_classifier_results.add_evaluation(y_test, clf_lm_pred)\n",
    "        em_classifier_results.add_evaluation(y_test, clf_em_pred)\n",
    "        dl_classifier_results.add_evaluation(y_test, clf_dl_pred)\n",
    "    \n",
    "    print(\"\\n The Models Results: \\n\")\n",
    "    lm_classifier_results.print_eval()\n",
    "    em_classifier_results.print_eval()\n",
    "    dl_classifier_results.print_eval()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the used classifiers\n",
    "clf_lm = LogisticRegression(max_iter=10000, solver='saga', tol=0.002)\n",
    "clf_em = RandomForestClassifier(criterion='entropy', max_depth=9, max_samples=0.9, n_estimators=100) \n",
    "clf_dl = MLPClassifier(max_iter=1000, solver='adam', activation='relu', alpha=0.005, hidden_layer_sizes=(7,3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = load_data(dataset_path)\n",
    "x_df, y_df = split_x_y_df(dataset_df)\n",
    "x_ndr_prcsd, y_ndr_prcsd = data_preprocess(x_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 out of 10 iterations.\n",
      "iteration 2 out of 10 iterations.\n",
      "iteration 3 out of 10 iterations.\n",
      "iteration 4 out of 10 iterations.\n",
      "iteration 5 out of 10 iterations.\n",
      "iteration 6 out of 10 iterations.\n",
      "iteration 7 out of 10 iterations.\n",
      "iteration 8 out of 10 iterations.\n",
      "iteration 9 out of 10 iterations.\n",
      "iteration 10 out of 10 iterations.\n",
      "\n",
      " The Models Results: \n",
      "\n",
      "Logistic Regression - Accuracy: 0.68288782830137 AUC: 0.6371409344006372 Percision: 0.11047514631941471 Recall: 0.5849170437405731 F1: 0.1857566597323889\n",
      "Random Forest - Accuracy: 0.8711508540060487 AUC: 0.5671732245526969 Percision: 0.14434995715384585 Recall: 0.220211161387632 F1: 0.1740507539831301\n",
      "Multi Layer Percepton - Accuracy: 0.5954499629959848 AUC: 0.5933811949882295 Percision: 0.0885519125054697 Recall: 0.5909879336349924 F1: 0.15341536179759235\n"
     ]
    }
   ],
   "source": [
    "run_experiment(x_ndr_prcsd, y_ndr_prcsd,clf_lm, clf_em, clf_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, as we can see from this comparison the Best recall we get is from the logistic regression model. although Random forest gets best accuracy, its recall is really low, which means we might predict wrongly about crucial cases. overall when weighing all these factors together, the best model In my opinion for this model is the Logistic Regression Models since it gets good results on most fields and is the easist to compute. On the other hand, it dpends on the type of the clinical trial, if accuracy is more important, Random Forest is better, but in this domain it feels the linear model works best. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
